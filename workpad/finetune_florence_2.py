# -*- coding: utf-8 -*-
"""Finetune-Florence-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/148doG2Lq65g4nxTXcmKp_MbHO4yPUWmB
"""

!pip install -q datasets flash_attn timm einops

!pip install datasets

import torch
from datasets import load_dataset

# data = load_dataset("HuggingFaceM4/DocumentVQA")

polis = load_dataset("Riksarkivet/goteborgs_poliskammare_fore_1900", trust_remote_code=True, name="text_recognition")

hovratt = load_dataset("Riksarkivet/gota_hovratt_seg", trust_remote_code=True)

# Load model
from transformers import AutoModelForCausalLM, AutoProcessor
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Florence-2-base-ft",
    trust_remote_code=True,
    revision='refs/pr/6'
).to(device)

processor = AutoProcessor.from_pretrained("microsoft/Florence-2-base-ft",
    trust_remote_code=True, revision='refs/pr/6')

for param in model.vision_tower.parameters():
  param.is_trainable = False

data["train"][0]

data["train"][0]["image"]

# Create VQA Dataset
import torch
from torch.utils.data import Dataset

class DocVQADataset(Dataset):

    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        example = self.data[idx]
        question = "<DocVQA>" + example['question']
        first_answer = example['answers'][0]
        image = example['image'].convert("RGB")
        return question, first_answer, image

# Create train loader & validate loader
import os
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import get_scheduler
from torch.optim import AdamW

# Processor comes from when loading the model
def collate_fn(batch):
    questions, answers, images = zip(*batch)
    inputs = processor(text=list(questions), images=list(images), return_tensors="pt", padding=True).to(device)
    return inputs, answers

train_dataset = DocVQADataset(data['train'].select(range(20)))
val_dataset = DocVQADataset(data['validation'].select(range(5)))
batch_size = 1
num_workers = 0

train_loader = DataLoader(train_dataset, batch_size=batch_size,
                          collate_fn=collate_fn, num_workers=num_workers, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size,
                          collate_fn=collate_fn, num_workers=num_workers)

epochs = 1
optimizer = AdamW(model.parameters(), lr=1e-6)
num_training_steps = epochs * len(train_loader)

lr_scheduler = get_scheduler(name="linear", optimizer=optimizer,
                              num_warmup_steps=0, num_training_steps=num_training_steps,)

for epoch in range(epochs):
    model.train()
    train_loss = 0
    i = -1

    # Inputs is the processed tuple (text, image)
    for inputs, answers in tqdm(train_loader, desc=f"Training Epoch {epoch + 1}/{epochs}"):
        i += 1

        # Get input (text tokens and )
        input_ids = inputs["input_ids"]
        pixel_values = inputs["pixel_values"]
        labels = processor.tokenizer(text=answers, return_tensors="pt", padding=True, return_token_type_ids=False).input_ids.to(device)

        # Predict output
        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)

        # Calculate loss, then backward
        loss = outputs.loss
        loss.backward()

        # Then step
        optimizer.step()
        lr_scheduler.step()

        # Reset grad
        optimizer.zero_grad()
        train_loss += loss.item()

    avg_train_loss = train_loss / len(train_loader)
    print(f"Average Training Loss: {avg_train_loss}")

# Check validation loss
model.eval()
val_loss = 0
with torch.no_grad():
    for batch in tqdm(val_loader, desc=f"Validation Epoch {epoch + 1}/{epochs}"):
        inputs, answers = batch
        input_ids = inputs["input_ids"]
        pixel_values = inputs["pixel_values"]
        labels = processor.tokenizer(text=answers, return_tensors="pt", padding=True, return_token_type_ids=False).input_ids.to(device)
        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)
        loss = outputs.loss
        val_loss += loss.item()

print(val_loss / len(val_loader))

from PIL import Image

image_path = "/content/drive/MyDrive/Projects/thesis-data/poliskammare-sample/small-sample/SCR-20250305-oefv.png"
image = Image.open(image_path).convert("RGB")

prompt = "<DocVQA>Print out the text in the image"

inputs = processor(text=prompt, images=image, return_tensors="pt").to(device)

generated_ids = model.generate(
    input_ids=inputs["input_ids"],
    pixel_values=inputs["pixel_values"],
    max_new_tokens=1024,
    do_sample=False,
    num_beams=3,
)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]

parsed_answer = processor.post_process_generation(generated_text, task="<VQA>", image_size=(image.width, image.height))

print(generated_text)
print(parsed_answer)

